{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Деревья решений решают проблемы\n",
    "__Суммарное количество баллов: 10__\n",
    "\n",
    "Вы уже знакомы с классификацией методом KNN. В этом задании предстоит реализовать другой метод классификации - дерево решений. \n",
    "\n",
    "Одной из его особенностей является возможность объяснить в человекочитаемой форме, почему мы отнесли объект к определенному классу. Эта особенность позволяет использовать деревья решений для создания систем, которые могут подсказывать специалистам, на что именно стоит обратить внимание при принятии решений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1 (1 балл)\n",
    "Во время построения дерева решений нам потребуется определить, какой из предикатов лучше всего разбивает обучающую выборку. Есть два критерия, которые позволяют это сделать: критерий Джини и энтропийный критерий. Первый для подсчета информативности разбиения использует коэффициент Джини, второй - энтропию. Реализуйте подсчет этих коэффициентов, а так же подсчет информативности разбиения. \n",
    "\n",
    "#### Описание функций\n",
    "`gini(x)` считает коэффициент Джини для массива меток\n",
    "\n",
    "`entropy(x)` считает энтропию для массива меток\n",
    "\n",
    "`gain(left_y, right_y, criterion)` считает информативность разбиения массива меток на левую `left_y` и правую `right_y` части при помощи `criterion`, который задается функцией (не строкой)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq(x):\n",
    "    count = np.bincount(x)\n",
    "    ind = np.nonzero(count)\n",
    "    return count[ind] / len(x)\n",
    "\n",
    "def gini(x=None, freq=None):\n",
    "    if freq is None and x is not None:\n",
    "        freq = get_freq(x)\n",
    "    return sum(freq * (1 - freq))\n",
    "    \n",
    "def entropy(x=None, freq=None):\n",
    "    if freq is None and x is not None:\n",
    "        freq = get_freq(x)\n",
    "    return - sum(freq * np.log2(freq))\n",
    "\n",
    "def gain(criterion=gini, left_y=None, right_y=None):\n",
    "    return -1 * (len(left_y) * criterion(left_y) + len(right_y) * criterion(right_y))\n",
    "\n",
    "# quick analog\n",
    "def gain2(criterion, right_sum, left_sum, right_freq, left_freq):\n",
    "    return  -1 * (left_sum * criterion(freq=left_freq) + right_sum * criterion(freq=right_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2 (1 балл)\n",
    "Деревья решений имеют хорошую интерпретируемость, т.к. позволяют не только предсказать класс, но и объяснить, почему мы предсказали именно его. Например, мы можем его нарисовать. Чтобы сделать это, нам необходимо знать, как оно устроено внутри. Реализуйте классы, которые будут задавать структуру дерева. \n",
    "\n",
    "#### DecisionTreeLeaf\n",
    "Поля:\n",
    "1. `y` должно содержать класс, который встречается чаще всего среди элементов листа дерева\n",
    "\n",
    "#### DecisionTreeNode\n",
    "В данной домашней работе мы ограничемся порядковыми и количественными признаками, поэтому достаточно хранить измерение и значение признака, по которому разбиваем обучающую выборку.\n",
    "\n",
    "Поля:\n",
    "1. `split_dim` измерение, по которому разбиваем выборку\n",
    "2. `split_value` значение, по которому разбираем выборку\n",
    "3. `left` поддерево, отвечающее за случай `x[split_dim] < split_value`. Может быть `DecisionTreeNode` или `DecisionTreeLeaf`\n",
    "4. `right` поддерево, отвечающее за случай `x[split_dim] >= split_value`. Может быть `DecisionTreeNode` или `DecisionTreeLeaf`\n",
    "\n",
    "__Интерфейс классов можно и нужно менять при необходимости__ (например, для вычисления вероятности в следующем задании)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeNode:\n",
    "    def __init__(self, split_dim=None, split_value=None, left=None, right=None, ind=[0], depth=0):\n",
    "        self.split_dim = None\n",
    "        self.split_value = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.ind = ind\n",
    "        self.size = len(ind)\n",
    "        self.depth = depth\n",
    "        \n",
    "    def define(self, split_dim, split_value, left, right):\n",
    "        self.split_dim = split_dim\n",
    "        self.split_value = split_value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "    \n",
    "    def isleaf(self):\n",
    "        return (self.left is None) and (self.right is None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3 (6 баллов)\n",
    "Теперь перейдем к самому дереву решений. Реализуйте класс `DecisionTreeClassifier`.\n",
    "\n",
    "#### Описание методов\n",
    "`fit(X, y)` строит дерево решений по обучающей выборке.\n",
    "\n",
    "`predict_proba(X)` для каждого элемента из `X` возвращает словарь `dict`, состоящий из пар `(класс, вероятность)`. Вероятности классов в листе можно определить через количество объектов соответствующего класса в листе. \n",
    "\n",
    "#### Описание параметров конструктора\n",
    "`criterion=\"gini\"` - задает критерий, который будет использоваться при построении дерева. Возможные значения: `\"gini\"`, `\"entropy\"`.\n",
    "\n",
    "`max_depth=None` - ограничение глубины дерева. Если `None` - глубина не ограничена\n",
    "\n",
    "`min_samples_leaf=1` - минимальное количество элементов в каждом листе дерева.\n",
    "\n",
    "#### Описание полей\n",
    "`root` - корень дерева. Может быть `DecisionTreeNode` или `DecisionTreeLeaf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guessing, y in [0, ..., N-1]\n",
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, criterion=\"gini\", max_depth=20, \n",
    "                 min_samples_leaf=1, max_iters=100, feat_bag=False, freq_feat_bag=0.8):\n",
    "        self.root = None\n",
    "        self.criterion = gini\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_depth = max_depth if max_depth is not None else np.inf\n",
    "        self.criterion = gini if criterion == \"gini\" else entropy\n",
    "        self.max_iters = max_iters\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.y = y\n",
    "        self.root = DecisionTreeNode(ind = np.linspace(0, len(X) - 1, len(X)).astype(int))\n",
    "        nodes = {self.root}\n",
    "\n",
    "        while nodes:\n",
    "            new_nodes = set()\n",
    "            for node in nodes:\n",
    "                if node.size > 2 * self.min_samples_leaf and node.depth < self.max_depth:\n",
    "                    split_dim, split_value = self.find_split(X[node.ind], y[node.ind])\n",
    "                    \n",
    "                    # if all y at same class\n",
    "                    if split_dim is None:\n",
    "                        continue\n",
    "                        \n",
    "                    sep = X[node.ind, split_dim] < split_value\n",
    "                    left_ind, right_ind = node.ind[sep], node.ind[~sep]\n",
    "                    \n",
    "                    # if all y at one side after separation\n",
    "                    if len(left_ind) == 0 or len(right_ind) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    left, right = self.make_nodes(split_dim, split_value, \n",
    "                                                  left_ind, right_ind, node)\n",
    "                    \n",
    "                    new_nodes.add(left)\n",
    "                    new_nodes.add(right)\n",
    "            nodes = new_nodes\n",
    "                        \n",
    "    def make_nodes(self, split_dim, split_value, left_ind, right_ind, node):\n",
    "        left = DecisionTreeNode(ind = left_ind, depth=node.depth + 1)\n",
    "        right = DecisionTreeNode(ind = right_ind, depth=node.depth + 1)\n",
    "        node.define(split_dim, split_value, left,  right)\n",
    "        return left, right\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        ind = np.linspace(0, len(X) - 1, len(X)).astype(int)\n",
    "        return self.traversal(X, ind, self.root)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return [max(p.keys(), key=lambda k: p[k]) for p in proba]\n",
    "    \n",
    "    def find_split(self, X, y):\n",
    "        quality = -np.inf\n",
    "        split_dim, split_value = -1, -1 # feature positin, impurity\n",
    "        con_fy = np.vstack([X[:, 0], y]).T # some feature, y\n",
    "        all_y = np.unique(y, return_counts=True)\n",
    "        \n",
    "        # If all y are same, do nothing\n",
    "        if len(all_y[0]) == 1:\n",
    "            return None, None\n",
    "            \n",
    "        left_counter = np.zeros(len(np.unique(y)))\n",
    "        right_counter =  np.zeros(len(np.unique(y)))\n",
    "        right_counter[all_y[0]] += all_y[1]\n",
    "\n",
    "        for feature in range(X.shape[1]):\n",
    "            con_fy[:, 0] = X[:, feature]\n",
    "            sort_fy = np.array(sorted(con_fy, key=lambda x: x[0])) # sort by feature\n",
    "\n",
    "            values_X, uniq_undexes = np.unique(sort_fy[:, 0], return_index=True)\n",
    "\n",
    "            # max interation is max_iters\n",
    "            if len(values_X) > self.max_iters:\n",
    "                indexes = sorted(np.random.choice(len(values_X), self.max_iters, replace=False))\n",
    "            else:\n",
    "                indexes = uniq_undexes\n",
    "\n",
    "            # clear counters\n",
    "            left_counter *= 0\n",
    "            right_counter *= 0\n",
    "            right_counter[all_y[0]] += all_y[1]\n",
    "            left_sum = 0\n",
    "            right_sum = len(y)\n",
    "\n",
    "            for i in range(0, len(indexes)): # examine y\n",
    "                if i == 0 and indexes[i - 1] != 0:\n",
    "                    ind1, ind2 = 0, indexes[i]\n",
    "                else:\n",
    "                    ind1, ind2 = indexes[i - 1], indexes[i]\n",
    "                y_values, counts = np.unique(sort_fy[ind1: ind2, 1], return_counts=True)\n",
    "\n",
    "                # update info about left and right\n",
    "                left_counter[y_values.astype(int)] += counts\n",
    "                right_counter[y_values.astype(int)] -= counts\n",
    "                left_sum += sum(counts)\n",
    "                right_sum -= sum(counts)\n",
    "                freq_left = left_counter / left_sum\n",
    "                freq_right = right_counter / right_sum\n",
    "                l, r = np.nonzero(freq_left), np.nonzero(freq_right)\n",
    "\n",
    "                # calculate gain\n",
    "                G = gain2(self.criterion, right_sum, left_sum, freq_right[r], freq_left[l])\n",
    "                \n",
    "                if G > quality:\n",
    "                    split_dim, split_value = feature, sort_fy[ind2, 0]\n",
    "                    quality = G\n",
    "                    \n",
    "        return split_dim, split_value\n",
    "    \n",
    "    def traversal(self, X, ind, node):\n",
    "        if node.isleaf():\n",
    "            values, counts = np.unique(self.y[node.ind], return_counts=True)\n",
    "            freq = counts / sum(counts)\n",
    "            y_pred = dict(zip(values, freq))\n",
    "            y_for_all = [y_pred for _ in range(len(ind))]\n",
    "            return y_for_all\n",
    "        \n",
    "        sep = X[ind, node.split_dim] < node.split_value\n",
    "        left_ind, right_ind = ind[sep], ind[~sep]\n",
    "        \n",
    "        y_pred = np.zeros(len(ind), dtype=object)\n",
    "        y_pred[sep] = self.traversal(X, left_ind, node.left)\n",
    "        y_pred[~sep] = self.traversal(X, right_ind, node.right)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построенное дерево можно нарисовать. Метод `draw_tree` рисует дерево и сохраняет его в указанный файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_depth(tree_root):\n",
    "    if isinstance(tree_root, DecisionTreeNode):\n",
    "        return max(tree_depth(tree_root.left), tree_depth(tree_root.right)) + 1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def draw_tree_rec(tree, tree_root, x_left, x_right, y):\n",
    "    x_center = (x_right - x_left) / 2 + x_left\n",
    "    if not tree_root.isleaf():\n",
    "        x_center = (x_right - x_left) / 2 + x_left\n",
    "        x = draw_tree_rec(tree, tree_root.left, x_left, x_center, y - 1)\n",
    "        plt.plot((x_center, x), (y - 0.1, y - 0.9), c=(0, 0, 0))\n",
    "        x = draw_tree_rec(tree, tree_root.right, x_center, x_right, y - 1)\n",
    "        plt.plot((x_center, x), (y - 0.1, y - 0.9), c=(0, 0, 0))\n",
    "        plt.text(x_center, y, \"x[%i] < %f\" % (tree_root.split_dim, tree_root.split_value),\n",
    "                horizontalalignment='center')\n",
    "    else:\n",
    "        y_pred = round(tree.y[tree_root.ind].mean())\n",
    "        plt.text(x_center, y, str(y_pred),\n",
    "                horizontalalignment='center')\n",
    "    return x_center\n",
    "\n",
    "def draw_tree(tree, save_path=None):\n",
    "    td = tree_depth(tree.root)\n",
    "    plt.figure(figsize=(0.33 * 2 ** td, 2 * td))\n",
    "    plt.xlim(-1, 1)\n",
    "    plt.ylim(0.95, td + 0.05)\n",
    "    plt.axis('off')\n",
    "    draw_tree_rec(tree, tree.root, -1, 1, td)\n",
    "    plt.tight_layout()\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для двумерного набора данных дерево можно отобразить на плоскости с данными. Кроме того, как и для любого классификатора, для него можно построить roc-кривую"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_test, p_pred):\n",
    "    positive_samples = sum(1 for y in y_test if y == 0)\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    for w in np.arange(-0.01, 1.02, 0.01):\n",
    "        y_pred = [(0 if p.get(0, 0) > w else 1) for p in p_pred]\n",
    "        tpr.append(sum(1 for yp, yt in zip(y_pred, y_test) if yp == 0 and yt == 0) / positive_samples)\n",
    "        fpr.append(sum(1 for yp, yt in zip(y_pred, y_test) if yp == 0 and yt != 0) / (len(y_test) - positive_samples))\n",
    "    plt.figure(figsize = (7, 7))\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False positive rate\")\n",
    "    plt.ylabel(\"True positive rate\")\n",
    "    plt.xlim(-0.01, 1.01)\n",
    "    plt.ylim(-0.01, 1.01)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def rectangle_bounds(bounds):\n",
    "    return ((bounds[0][0], bounds[0][0], bounds[0][1], bounds[0][1]), \n",
    "            (bounds[1][0], bounds[1][1], bounds[1][1], bounds[1][0]))\n",
    "\n",
    "def plot_2d_tree(tree_root, bounds, colors):\n",
    "    if isinstance(tree_root, DecisionTreeNode):\n",
    "        if tree_root.split_dim:\n",
    "            plot_2d_tree(tree_root.left, [bounds[0], [bounds[1][0], tree_root.split_value]], colors)\n",
    "            plot_2d_tree(tree_root.right, [bounds[0], [tree_root.split_value, bounds[1][1]]], colors)\n",
    "            plt.plot(bounds[0], (tree_root.split_value, tree_root.split_value), c=(0, 0, 0))\n",
    "        else:\n",
    "            plot_2d_tree(tree_root.left, [[bounds[0][0], tree_root.split_value], bounds[1]], colors)\n",
    "            plot_2d_tree(tree_root.right, [[tree_root.split_value, bounds[0][1]], bounds[1]], colors)\n",
    "            plt.plot((tree_root.split_value, tree_root.split_value), bounds[1], c=(0, 0, 0))\n",
    "    else:\n",
    "        x, y = rectangle_bounds(bounds)\n",
    "        plt.fill(x, y, c=colors[tree_root.y] + [0.2])\n",
    "\n",
    "def plot_2d(tree, X, y):\n",
    "    plt.figure(figsize=(9, 9))\n",
    "    colors = dict((c, list(np.random.random(3))) for c in np.unique(y))\n",
    "    bounds = list(zip(np.min(X, axis=0), np.max(X, axis=0)))\n",
    "    plt.xlim(*bounds[0])\n",
    "    plt.ylim(*bounds[1])\n",
    "    plot_2d_tree(tree.root, list(zip(np.min(X, axis=0), np.max(X, axis=0))), colors)\n",
    "    for c in np.unique(y):\n",
    "        plt.scatter(X[y==c, 0], X[y==c, 1], c=[colors[c]], label=c)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4 (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируйте решение на данных cancer и spam (датасеты из директории `hw2_data`).\n",
    "Выполните загрузку и предобработку данных.\n",
    "\n",
    "\n",
    "Если необходимо, попробуйте разные наборы параметров для получения лучшего результата.\n",
    "\n",
    "Посчитайте метрики `precision`, `recall`, `accuracy` для модели дерева.\n",
    "Сравните значения метрик с результатами модели kNN из предыдущего задания (можно использовать реализацию из библиотеки `sklearn`). \n",
    "\n",
    "Какой нужен препроцессинг данных для моделей?\n",
    "Какая модель делает предсказания лучше?  Предположите, почему."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNeighborsClassifier - сработал хорошо прямо из коробки(без подбора параметров). Для этого алгоритма нужно делать скейлинг. Для дерева, скейлинг не обязателен, так как условия больше или меньше не изменятся при нем. Предсказания +- одинаковые на 2ух моделях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, \\\n",
    "                            recall_score, precision_score\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cancer_dataset(path_to_csv):\n",
    "    # Возвращает пару из X и y. X - массив векторов. y - соответствующие векторам метки\n",
    "    df = shuffle(pd.read_csv(path_to_csv))\n",
    "    y = (df['label'].values == \"M\").astype(int)\n",
    "    X = df.drop('label', axis=1)\n",
    "    return X, y\n",
    "\n",
    "def read_spam_dataset(path_to_csv):\n",
    "    # Возвращает пару из X и y. X - массив векторов. y - соответствующие векторам метки\n",
    "    df = shuffle(pd.read_csv(path_to_csv))\n",
    "    y = df['label'].values\n",
    "    X = df.drop('label', axis=1)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-ec187c1d12e2>:99: RuntimeWarning: invalid value encountered in true_divide\n",
      "  freq_left = left_counter / left_sum\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score as roc\n",
    "\n",
    "scale = StandardScaler()\n",
    "\n",
    "X, y = read_spam_dataset(\"hw2_data/spam.csv\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "scale.fit(X_train)\n",
    "X_train, X_test = scale.transform(X_train), scale.transform(X_test)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "knn_pred = knn.predict(X_test)\n",
    "knn_pred_proba = [proba[1] for proba in knn.predict_proba(X_test)]\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=6, min_samples_leaf=40, max_iters=600, criterion=\"entropy\")\n",
    "tree.fit(X_train, y_train)\n",
    "tree_pred = tree.predict(X_test)\n",
    "tree_pred_proba = np.zeros_like(y_test).astype(\"float\")\n",
    "\n",
    "for i, proba in enumerate(tree.predict_proba(X_test)):\n",
    "    if 1 in proba.keys():\n",
    "        tree_pred_proba[i] = proba[1]\n",
    "    else:\n",
    "        tree_pred_proba[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam\n",
      "auc: tree - 0.9052997393570807, knn - 0.894005212858384\n",
      "recall: tree - 0.908695652173913, knn - 0.9006622516556292\n",
      "precision: tree - 0.8618556701030928, knn - 0.8412371134020619\n",
      "roc: tree - 0.9491037429181759, knn - 0.949839014272004\n"
     ]
    }
   ],
   "source": [
    "print(\"Spam\")\n",
    "print(\"auc:\", f\"tree - {accuracy_score(tree_pred, y_test)},\", \n",
    "              f\"knn - {accuracy_score(knn_pred, y_test)}\")\n",
    "\n",
    "print(\"recall:\", f\"tree - {recall_score(tree_pred, y_test)},\", \n",
    "              f\"knn - {recall_score(knn_pred, y_test)}\")\n",
    "\n",
    "print(\"precision:\", f\"tree - {precision_score(tree_pred, y_test)},\", \n",
    "              f\"knn - {precision_score(knn_pred, y_test)}\")\n",
    "\n",
    "print(\"roc:\", f\"tree - {roc(y_test, tree_pred_proba)},\",\n",
    "              f\"knn - {roc(y_test, knn_pred_proba)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-ec187c1d12e2>:99: RuntimeWarning: invalid value encountered in true_divide\n",
      "  freq_left = left_counter / left_sum\n"
     ]
    }
   ],
   "source": [
    "scale = StandardScaler()\n",
    "\n",
    "X, y = read_cancer_dataset(\"hw2_data/cancer.csv\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "scale.fit(X_train)\n",
    "X_train, X_test = scale.transform(X_train), scale.transform(X_test)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "knn_pred = knn.predict(X_test)\n",
    "knn_pred_proba = [proba[1] for proba in knn.predict_proba(X_test)]\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=4, min_samples_leaf=20, max_iters=600, criterion=\"gini\")\n",
    "tree.fit(X_train, y_train)\n",
    "tree_pred = tree.predict(X_test)\n",
    "tree_pred_proba = np.zeros_like(y_test).astype(\"float\")\n",
    "\n",
    "for i, proba in enumerate(tree.predict_proba(X_test)):\n",
    "    if 1 in proba.keys():\n",
    "        tree_pred_proba[i] = proba[1]\n",
    "    else:\n",
    "        tree_pred_proba[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cancer\n",
      "auc: tree - 0.8951048951048951, knn - 0.986013986013986\n",
      "recall: tree - 0.9777777777777777, knn - 1.0\n",
      "precision: tree - 0.7586206896551724, knn - 0.9655172413793104\n",
      "roc: tree - 0.9842799188640974, knn - 0.9892494929006086\n"
     ]
    }
   ],
   "source": [
    "print(\"Cancer\")\n",
    "print(\"auc:\", f\"tree - {accuracy_score(tree_pred, y_test)},\", \n",
    "              f\"knn - {accuracy_score(knn_pred, y_test)}\")\n",
    "\n",
    "print(\"recall:\", f\"tree - {recall_score(tree_pred, y_test)},\", \n",
    "              f\"knn - {recall_score(knn_pred, y_test)}\")\n",
    "\n",
    "print(\"precision:\", f\"tree - {precision_score(tree_pred, y_test)},\", \n",
    "              f\"knn - {precision_score(knn_pred, y_test)}\")\n",
    "\n",
    "print(\"roc:\", f\"tree - {roc(y_test, tree_pred_proba)},\",\n",
    "              f\"knn - {roc(y_test, knn_pred_proba)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
